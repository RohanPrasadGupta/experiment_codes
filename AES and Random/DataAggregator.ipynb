{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cddf12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd67a530",
   "metadata": {},
   "source": [
    "# Interval of 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6478299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the folder path containing Base64 encoded files for decryption: C:/Users/ROHAN/IotSimulation/Base64\n",
      "Enter the folder path to save merged data: C:/Users/ROHAN/IotSimulation/DecForAggreator\n",
      "Merged files saved in C:/Users/ROHAN/IotSimulation/DecForAggreator\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def decrypt_ct_prime(ct_prime):\n",
    "    R1, encrypted_data, R2 = ct_prime.split('||')\n",
    "    json_data = encrypted_data[len(\"Encrypted(\"):-1]\n",
    "    decrypted_data = json.loads(json_data)\n",
    "    return decrypted_data\n",
    "\n",
    "def read_and_decrypt_files(folder_path_base64, folder_path_merged):\n",
    "    if not os.path.exists(folder_path_merged):\n",
    "        os.makedirs(folder_path_merged)\n",
    "\n",
    "    user_data = {}\n",
    "\n",
    "    for filename in os.listdir(folder_path_base64):\n",
    "        if filename.endswith('_base64.txt'):\n",
    "            file_path = os.path.join(folder_path_base64, filename)\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                encoded_data = file.read()\n",
    "                padded_encoded_data = encoded_data + '==='\n",
    "                try:\n",
    "                    decoded_data = base64.b64decode(padded_encoded_data).decode('utf-8')\n",
    "                    decrypted_info = decrypt_ct_prime(decoded_data)\n",
    "\n",
    "                    user_id = decrypted_info['user_id']\n",
    "                    timestamp = datetime.strptime(decrypted_info['timestamp'], \"%Y-%m-%d %H_%M_%S\")\n",
    "                    if user_id not in user_data:\n",
    "                        user_data[user_id] = []\n",
    "                    user_data[user_id].append((timestamp, decrypted_info))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error decoding file {filename}: {e}\")\n",
    "\n",
    "    for user_id, data in user_data.items():\n",
    "        data.sort(key=lambda x: x[0])  # Sort by timestamp\n",
    "        merged_data = []\n",
    "        current_batch = []\n",
    "        current_batch_time = None\n",
    "\n",
    "        for timestamp, info in data:\n",
    "            if current_batch_time is None or timestamp - current_batch_time <= timedelta(minutes=5):\n",
    "                current_batch.append(info)\n",
    "            else:\n",
    "                merged_data.append(current_batch)\n",
    "                current_batch = [info]\n",
    "            current_batch_time = timestamp\n",
    "\n",
    "        if current_batch:\n",
    "            merged_data.append(current_batch)\n",
    "\n",
    "        save_merged_data(merged_data, user_id, folder_path_merged)\n",
    "\n",
    "def save_merged_data(merged_data, user_id, folder_path):\n",
    "    for i, batch in enumerate(merged_data):\n",
    "        merged_filename = f\"user_{user_id}_merged_{i+1}.txt\"\n",
    "        merged_file_path = os.path.join(folder_path, merged_filename)\n",
    "        with open(merged_file_path, 'w') as merged_file:\n",
    "            json.dump(batch, merged_file, indent=4)\n",
    "\n",
    "def main():\n",
    "    folder_path_base64 = input(\"Enter the folder path containing Base64 encoded files for decryption: \")\n",
    "    folder_path_merged = input(\"Enter the folder path to save merged data: \")\n",
    "    read_and_decrypt_files(folder_path_base64, folder_path_merged)\n",
    "    print(f\"Merged files saved in {folder_path_merged}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8136cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66595cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:/Users/ROHAN/IotSimulation/Base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011bf825",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:/Users/ROHAN/IotSimulation/DecForAggreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a69af96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e61894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interval of 20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb5f0f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the folder path containing Base64 encoded files for decryption: C:/Users/ROHAN/IotSimulation/Base64\n",
      "Enter the folder path to save merged data: C:/Users/ROHAN/IotSimulation/DecForAggreator\n",
      "Merged files saved in C:/Users/ROHAN/IotSimulation/DecForAggreator\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def decrypt_ct_prime(ct_prime):\n",
    "    R1, encrypted_data, R2 = ct_prime.split('||')\n",
    "    json_data = encrypted_data[len(\"Encrypted(\"):-1]\n",
    "    decrypted_data = json.loads(json_data)\n",
    "    return decrypted_data\n",
    "\n",
    "def read_and_decrypt_files(folder_path_base64, folder_path_merged):\n",
    "    if not os.path.exists(folder_path_merged):\n",
    "        os.makedirs(folder_path_merged)\n",
    "\n",
    "    user_data = {}\n",
    "\n",
    "    for filename in os.listdir(folder_path_base64):\n",
    "        if filename.endswith('_base64.txt'):\n",
    "            file_path = os.path.join(folder_path_base64, filename)\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                encoded_data = file.read()\n",
    "                padded_encoded_data = encoded_data + '==='\n",
    "                try:\n",
    "                    decoded_data = base64.b64decode(padded_encoded_data).decode('utf-8')\n",
    "                    decrypted_info = decrypt_ct_prime(decoded_data)\n",
    "\n",
    "                    user_id = decrypted_info['user_id']\n",
    "                    timestamp = datetime.strptime(decrypted_info['timestamp'], \"%Y-%m-%d %H_%M_%S\")\n",
    "                    if user_id not in user_data:\n",
    "                        user_data[user_id] = []\n",
    "                    user_data[user_id].append((timestamp, decrypted_info))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error decoding file {filename}: {e}\")\n",
    "\n",
    "    for user_id, data in user_data.items():\n",
    "        data.sort(key=lambda x: x[0])  # Sort by timestamp\n",
    "        merged_data = []\n",
    "        current_batch = []\n",
    "        current_batch_time = None\n",
    "\n",
    "        for timestamp, info in data:\n",
    "            if current_batch_time is None or timestamp - current_batch_time <= timedelta(minutes=20):\n",
    "                current_batch.append(info)\n",
    "            else:\n",
    "                merged_data.append(current_batch)\n",
    "                current_batch = [info]\n",
    "            current_batch_time = timestamp\n",
    "\n",
    "        if current_batch:\n",
    "            merged_data.append(current_batch)\n",
    "\n",
    "        save_merged_data(merged_data, user_id, folder_path_merged)\n",
    "\n",
    "def save_merged_data(merged_data, user_id, folder_path):\n",
    "    for i, batch in enumerate(merged_data):\n",
    "        merged_filename = f\"user_{user_id}_merged_{i+1}.txt\"\n",
    "        merged_file_path = os.path.join(folder_path, merged_filename)\n",
    "        with open(merged_file_path, 'w') as merged_file:\n",
    "            json.dump(batch, merged_file, indent=4)\n",
    "\n",
    "def main():\n",
    "    folder_path_base64 = input(\"Enter the folder path containing Base64 encoded files for decryption: \")\n",
    "    folder_path_merged = input(\"Enter the folder path to save merged data: \")\n",
    "    read_and_decrypt_files(folder_path_base64, folder_path_merged)\n",
    "    print(f\"Merged files saved in {folder_path_merged}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d127e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61fb9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07a822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1884f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52b814df",
   "metadata": {},
   "source": [
    "# interval with 20 minutes and report for both decryption and aggretion in seperate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a396b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a34885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the folder path containing Base64 encoded files for decryption: C:/Users/ROHAN/IotSimulation/Base64\n",
      "Enter the folder path to save merged data: C:/Users/ROHAN/IotSimulation/DecForAggreator\n",
      "Enter the directory for the reports: C:/Users/ROHAN/IotSimulation\n",
      "Merged files saved in C:/Users/ROHAN/IotSimulation/DecForAggreator\n",
      "Reports saved in C:/Users/ROHAN/IotSimulation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from time import perf_counter\n",
    "\n",
    "def decrypt_ct_prime(ct_prime, decryption_report, filename):\n",
    "    start_decryption_time = perf_counter()\n",
    "    \n",
    "    R1, encrypted_data, R2 = ct_prime.split('||')\n",
    "    json_data = encrypted_data[len(\"Encrypted(\"):-1]\n",
    "    decrypted_data = json.loads(json_data)\n",
    "\n",
    "    end_decryption_time = perf_counter()\n",
    "    decryption_duration = end_decryption_time - start_decryption_time\n",
    "\n",
    "    decryption_report.append({\n",
    "        'filename': filename,\n",
    "        'decryption_duration': decryption_duration\n",
    "    })\n",
    "\n",
    "    return decrypted_data\n",
    "\n",
    "def save_merged_data(merged_data, user_id, folder_path, merge_report):\n",
    "    for i, batch in enumerate(merged_data):\n",
    "        start_merge_time = perf_counter()\n",
    "        merged_filename = f\"user_{user_id}_merged_{i+1}.txt\"\n",
    "        merged_file_path = os.path.join(folder_path, merged_filename)\n",
    "\n",
    "        with open(merged_file_path, 'w') as merged_file:\n",
    "            json.dump(batch, merged_file, indent=4)\n",
    "\n",
    "        end_merge_time = perf_counter()\n",
    "        merge_duration = end_merge_time - start_merge_time\n",
    "        merged_file_size = os.path.getsize(merged_file_path)\n",
    "\n",
    "        individual_files = [data['filename'] for data in batch]\n",
    "        individual_sizes = [data['file_size'] for data in batch]\n",
    "\n",
    "        merge_report.append({\n",
    "            'user_id': user_id,\n",
    "            'merged_file': merged_filename,\n",
    "            'merge_duration': merge_duration,\n",
    "            'merged_file_size': merged_file_size,\n",
    "            'individual_files': individual_files,\n",
    "            'individual_sizes': individual_sizes\n",
    "        })\n",
    "\n",
    "def read_and_decrypt_files(folder_path_base64, folder_path_merged, merge_report, decryption_report):\n",
    "    if not os.path.exists(folder_path_merged):\n",
    "        os.makedirs(folder_path_merged)\n",
    "\n",
    "    user_data = {}\n",
    "\n",
    "    for filename in os.listdir(folder_path_base64):\n",
    "        if filename.endswith('_base64.txt'):\n",
    "            file_path = os.path.join(folder_path_base64, filename)\n",
    "            file_size = os.path.getsize(file_path)\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                encoded_data = file.read()\n",
    "                padded_encoded_data = encoded_data + '==='\n",
    "                try:\n",
    "                    decoded_data = base64.b64decode(padded_encoded_data).decode('utf-8')\n",
    "                    decrypted_info = decrypt_ct_prime(decoded_data, decryption_report, filename)\n",
    "\n",
    "                    user_id = decrypted_info['user_id']\n",
    "                    timestamp = datetime.strptime(decrypted_info['timestamp'], \"%Y-%m-%d %H_%M_%S\")\n",
    "                    if user_id not in user_data:\n",
    "                        user_data[user_id] = []\n",
    "                    decrypted_info['filename'] = filename\n",
    "                    decrypted_info['file_size'] = file_size\n",
    "                    user_data[user_id].append((timestamp, decrypted_info))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error decoding file {filename}: {e}\")\n",
    "\n",
    "    for user_id, data in user_data.items():\n",
    "        data.sort(key=lambda x: x[0])  # Sort by timestamp\n",
    "        merged_data = []\n",
    "        current_batch = []\n",
    "        current_batch_time = None\n",
    "\n",
    "        for timestamp, info in data:\n",
    "            if current_batch_time is None or timestamp - current_batch_time <= timedelta(minutes=20):\n",
    "                current_batch.append(info)\n",
    "            else:\n",
    "                merged_data.append(current_batch)\n",
    "                current_batch = [info]\n",
    "            current_batch_time = timestamp\n",
    "\n",
    "        if current_batch:\n",
    "            merged_data.append(current_batch)\n",
    "\n",
    "        save_merged_data(merged_data, user_id, folder_path_merged, merge_report)\n",
    "\n",
    "def write_merge_report(merge_report, report_path):\n",
    "    merge_report_path = os.path.join(report_path, \"merge_report.csv\")\n",
    "    with open(merge_report_path, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['user_id', 'merged_file', 'merge_duration', 'merged_file_size', 'individual_files', 'individual_sizes'])\n",
    "        writer.writeheader()\n",
    "        for entry in merge_report:\n",
    "            writer.writerow(entry)\n",
    "\n",
    "def write_decryption_report(decryption_report, report_path):\n",
    "    decryption_report_path = os.path.join(report_path, \"decryption_report.csv\")\n",
    "    with open(decryption_report_path, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['filename', 'decryption_duration'])\n",
    "        writer.writeheader()\n",
    "        for entry in decryption_report:\n",
    "            writer.writerow(entry)\n",
    "\n",
    "def main():\n",
    "    folder_path_base64 = input(\"Enter the folder path containing Base64 encoded files for decryption: \")\n",
    "    folder_path_merged = input(\"Enter the folder path to save merged data: \")\n",
    "    report_path = input(\"Enter the directory for the reports: \")\n",
    "\n",
    "    merge_report = []\n",
    "    decryption_report = []\n",
    "    read_and_decrypt_files(folder_path_base64, folder_path_merged, merge_report, decryption_report)\n",
    "    write_merge_report(merge_report, report_path)\n",
    "    write_decryption_report(decryption_report, report_path)\n",
    "\n",
    "    print(f\"Merged files saved in {folder_path_merged}\")\n",
    "    print(f\"Reports saved in {report_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe93721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aefc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:/Users/ROHAN/IotSimulation/Base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057f7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:/Users/ROHAN/IotSimulation/DecForAggreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e787b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4704d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d673135e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec3c0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff006a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the folder path containing Base64 encoded files for decryption: C:/Users/ROHAN/IotSimulation/Base64\n",
      "Enter the folder path to save merged data: C:/Users/ROHAN/IotSimulation/DecForAggreator\n",
      "Enter the directory for the reports: C:/Users/ROHAN/IotSimulation\n",
      "Merged files saved in C:/Users/ROHAN/IotSimulation/DecForAggreator\n",
      "Reports saved in C:/Users/ROHAN/IotSimulation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from time import perf_counter\n",
    "\n",
    "def decrypt_ct_prime(ct_prime, decryption_report, filename):\n",
    "    start_decryption_time = perf_counter()\n",
    "    \n",
    "    R1, encrypted_data, R2 = ct_prime.split('||')\n",
    "    json_data = encrypted_data[len(\"Encrypted(\"):-1]\n",
    "    decrypted_data = json.loads(json_data)\n",
    "    user_id = decrypted_data['user_id']  # Extract user_id from decrypted data\n",
    "\n",
    "    end_decryption_time = perf_counter()\n",
    "    decryption_duration = end_decryption_time - start_decryption_time\n",
    "\n",
    "    decryption_report.append({\n",
    "        'filename': filename,\n",
    "        'user_id': user_id,  # Include user_id in the report\n",
    "        'decryption_duration': decryption_duration\n",
    "    })\n",
    "\n",
    "    return decrypted_data, user_id  # Return both decrypted data and user_id\n",
    "\n",
    "def save_merged_data(merged_data, user_id, folder_path, merge_report):\n",
    "    for i, batch in enumerate(merged_data):\n",
    "        start_merge_time = perf_counter()\n",
    "\n",
    "        authorized_users = batch[0]['authorized_data_users']\n",
    "        authorized_users_str = '_'.join(authorized_users)\n",
    "        \n",
    "        merged_filename = f\"user_{user_id}_auth_{authorized_users_str}_merged_{i+1}.txt\"\n",
    "        merged_file_path = os.path.join(folder_path, merged_filename)\n",
    "\n",
    "        with open(merged_file_path, 'w') as merged_file:\n",
    "            json.dump(batch, merged_file, indent=4)\n",
    "\n",
    "        end_merge_time = perf_counter()\n",
    "        merge_duration = end_merge_time - start_merge_time\n",
    "        merged_file_size = os.path.getsize(merged_file_path)\n",
    "\n",
    "        individual_files = [data['filename'] for data in batch]\n",
    "        individual_sizes = [data['file_size'] for data in batch]\n",
    "\n",
    "        merge_report.append({\n",
    "            'user_id': user_id,\n",
    "            'merged_file': merged_filename,\n",
    "            'merge_duration': merge_duration,\n",
    "            'merged_file_size': merged_file_size,\n",
    "            'individual_files': individual_files,\n",
    "            'individual_sizes': individual_sizes\n",
    "        })\n",
    "\n",
    "def read_and_decrypt_files(folder_path_base64, folder_path_merged, merge_report, decryption_report):\n",
    "    if not os.path.exists(folder_path_merged):\n",
    "        os.makedirs(folder_path_merged)\n",
    "\n",
    "    user_data = {}\n",
    "\n",
    "    for filename in os.listdir(folder_path_base64):\n",
    "        if filename.endswith('_base64.txt'):\n",
    "            file_path = os.path.join(folder_path_base64, filename)\n",
    "            file_size = os.path.getsize(file_path)\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                encoded_data = file.read()\n",
    "                padded_encoded_data = encoded_data + '==='\n",
    "                try:\n",
    "                    decoded_data = base64.b64decode(padded_encoded_data).decode('utf-8')\n",
    "                    decrypted_info, user_id = decrypt_ct_prime(decoded_data, decryption_report, filename)  # Capture user_id\n",
    "\n",
    "                    timestamp = datetime.strptime(decrypted_info['timestamp'], \"%Y-%m-%d %H_%M_%S\")\n",
    "                    if user_id not in user_data:\n",
    "                        user_data[user_id] = []\n",
    "                    decrypted_info['filename'] = filename\n",
    "                    decrypted_info['file_size'] = file_size\n",
    "                    user_data[user_id].append((timestamp, decrypted_info))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error decoding file {filename}: {e}\")\n",
    "\n",
    "    for user_id, data in user_data.items():\n",
    "        data.sort(key=lambda x: x[0])  # Sort by timestamp\n",
    "        merged_data = []\n",
    "        current_batch = []\n",
    "        current_batch_time = None\n",
    "\n",
    "        for timestamp, info in data:\n",
    "            if current_batch_time is None or timestamp - current_batch_time <= timedelta(minutes=20):\n",
    "                current_batch.append(info)\n",
    "            else:\n",
    "                merged_data.append(current_batch)\n",
    "                current_batch = [info]\n",
    "            current_batch_time = timestamp\n",
    "\n",
    "        if current_batch:\n",
    "            merged_data.append(current_batch)\n",
    "\n",
    "        save_merged_data(merged_data, user_id, folder_path_merged, merge_report)\n",
    "\n",
    "def write_merge_report(merge_report, report_path):\n",
    "    merge_report_path = os.path.join(report_path, \"merge_report.csv\")\n",
    "    with open(merge_report_path, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['user_id', 'merged_file', 'merge_duration', 'merged_file_size', 'individual_files', 'individual_sizes'])\n",
    "        writer.writeheader()\n",
    "        for entry in merge_report:\n",
    "            writer.writerow(entry)\n",
    "\n",
    "def write_decryption_report(decryption_report, report_path):\n",
    "    decryption_report_path = os.path.join(report_path, \"decryption_report.csv\")\n",
    "    with open(decryption_report_path, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['filename', 'user_id', 'decryption_duration'])\n",
    "        writer.writeheader()\n",
    "        for entry in decryption_report:\n",
    "            writer.writerow(entry)\n",
    "\n",
    "def main():\n",
    "    folder_path_base64 = input(\"Enter the folder path containing Base64 encoded files for decryption: \")\n",
    "    folder_path_merged = input(\"Enter the folder path to save merged data: \")\n",
    "    report_path = input(\"Enter the directory for the reports: \")\n",
    "\n",
    "    merge_report = []\n",
    "    decryption_report = []\n",
    "    read_and_decrypt_files(folder_path_base64, folder_path_merged, merge_report, decryption_report)\n",
    "    write_merge_report(merge_report, report_path)\n",
    "    write_decryption_report(decryption_report, report_path)\n",
    "\n",
    "    print(f\"Merged files saved in {folder_path_merged}\")\n",
    "    print(f\"Reports saved in {report_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77e524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa27f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:/Users/ROHAN/IotSimulation/Base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef310af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:/Users/ROHAN/IotSimulation/DecForAggreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e82d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331112a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11921f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88f8f9a6",
   "metadata": {},
   "source": [
    "# Data Aggregation For Cornference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d37f0",
   "metadata": {},
   "source": [
    "- File Size 50KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81994022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the folder path containing the data files: C:/Users/ROHAN/IotSimulation/ConReq/IoTDataTxtFile\n",
      "Enter the folder path to save aggregated data: C:/Users/ROHAN/IotSimulation/ConReq/AggData50kb\n",
      "Enter the directory for the aggregation report: C:/Users/ROHAN/IotSimulation/ConReq\n",
      "Aggregated files saved in C:/Users/ROHAN/IotSimulation/ConReq/AggData50kb\n",
      "Report saved in C:/Users/ROHAN/IotSimulation/ConReq\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "\n",
    "def aggregate_data(input_folder_path, output_folder_path, aggregate_report):\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    user_data = {}\n",
    "\n",
    "    for filename in os.listdir(input_folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(input_folder_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                user_id = data['user_id']\n",
    "\n",
    "                if user_id not in user_data:\n",
    "                    user_data[user_id] = []\n",
    "                user_data[user_id].append(data)\n",
    "\n",
    "    for user_id, data in user_data.items():\n",
    "        aggregated_filename = f\"user_{user_id}_aggregated.txt\"\n",
    "        aggregated_file_path = os.path.join(output_folder_path, aggregated_filename)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        serialized_data = json.dumps(data, indent=4)\n",
    "        while len(serialized_data.encode('utf-8')) < 51200:  # Targeting approximately 50KB\n",
    "            dummy_data = {\"extra\": ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=1000))}\n",
    "            data.append(dummy_data)\n",
    "            serialized_data = json.dumps(data, indent=4)\n",
    "\n",
    "        with open(aggregated_file_path, 'w') as aggregated_file:\n",
    "            aggregated_file.write(serialized_data)\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        aggregation_duration = end_time - start_time\n",
    "        file_size = os.path.getsize(aggregated_file_path)\n",
    "\n",
    "        aggregate_report.append({\n",
    "            'user_id': user_id,\n",
    "            'aggregated_file': aggregated_filename,\n",
    "            'number_of_records': len(data),\n",
    "            'file_size': file_size,\n",
    "            'aggregation_duration': aggregation_duration\n",
    "        })\n",
    "\n",
    "def write_aggregate_report(aggregate_report, report_path):\n",
    "    if not os.path.exists(report_path):\n",
    "        os.makedirs(report_path)\n",
    "\n",
    "    aggregate_report_path = os.path.join(report_path, \"aggregate_report_50kb.csv\")\n",
    "    with open(aggregate_report_path, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['user_id', 'aggregated_file', 'number_of_records', 'file_size', 'aggregation_duration'])\n",
    "        writer.writeheader()\n",
    "        for entry in aggregate_report:\n",
    "            writer.writerow(entry)\n",
    "\n",
    "def main():\n",
    "    input_folder_path = input(\"Enter the folder path containing the data files: \")\n",
    "    output_folder_path = input(\"Enter the folder path to save aggregated data: \")\n",
    "    report_path = input(\"Enter the directory for the aggregation report: \")\n",
    "\n",
    "    aggregate_report = []\n",
    "    aggregate_data(input_folder_path, output_folder_path, aggregate_report)\n",
    "    write_aggregate_report(aggregate_report, report_path)\n",
    "\n",
    "    print(f\"Aggregated files saved in {output_folder_path}\")\n",
    "    print(f\"Report saved in {report_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe3ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722dfbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:/Users/ROHAN/IotSimulation/DATASETS/DataSet4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c2bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:/Users/ROHAN/IotSimulation/DecForAggreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:/Users/ROHAN/IotSimulation/ConReq/IoTDataTxtFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb73ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:/Users/ROHAN/IotSimulation/ConReq/AggData50kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7b645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8587130f",
   "metadata": {},
   "source": [
    "# Data Aggreagator For Conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d4960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18687681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from time import perf_counter\n",
    "\n",
    "def decrypt_ct_prime(ct_prime, decryption_report, filename):\n",
    "    start_decryption_time = perf_counter()\n",
    "    \n",
    "    R1, encrypted_data, R2 = ct_prime.split('||')\n",
    "    json_data = encrypted_data[len(\"Encrypted(\"):-1]\n",
    "    decrypted_data = json.loads(json_data)\n",
    "    user_id = decrypted_data['user_id']  # Extract user_id from decrypted data\n",
    "\n",
    "    end_decryption_time = perf_counter()\n",
    "    decryption_duration = end_decryption_time - start_decryption_time\n",
    "\n",
    "    decryption_report.append({\n",
    "        'filename': filename,\n",
    "        'user_id': user_id,  # Include user_id in the report\n",
    "        'decryption_duration': decryption_duration\n",
    "    })\n",
    "\n",
    "    return decrypted_data, user_id  # Return both decrypted data and user_id\n",
    "\n",
    "def save_merged_data(merged_data, user_id, folder_path, merge_report):\n",
    "    for i, batch in enumerate(merged_data):\n",
    "        start_merge_time = perf_counter()\n",
    "\n",
    "        authorized_users = batch[0]['authorized_data_users']\n",
    "        authorized_users_str = '_'.join(authorized_users)\n",
    "        \n",
    "        merged_filename = f\"user_{user_id}_auth_{authorized_users_str}_merged_{i+1}.txt\"\n",
    "        merged_file_path = os.path.join(folder_path, merged_filename)\n",
    "\n",
    "        with open(merged_file_path, 'w') as merged_file:\n",
    "            serialized_data = json.dumps(batch, indent=4)\n",
    "            target_file_size = 1024 * 1024  # 1 MB in bytes\n",
    "            while len(serialized_data.encode('utf-8')) < target_file_size:\n",
    "                # Generate random data\n",
    "                random_data = {'extra': ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=1000))}\n",
    "                batch.append(random_data)\n",
    "                serialized_data = json.dumps(batch, indent=4)\n",
    "\n",
    "            merged_file.write(serialized_data)\n",
    "\n",
    "        end_merge_time = perf_counter()\n",
    "        merge_duration = end_merge_time - start_merge_time\n",
    "        merged_file_size = os.path.getsize(merged_file_path)\n",
    "\n",
    "        individual_files = [data.get('filename', 'Unknown') for data in batch]\n",
    "        individual_sizes = [data['file_size'] for data in batch]\n",
    "\n",
    "        merge_report.append({\n",
    "            'user_id': user_id,\n",
    "            'merged_file': merged_filename,\n",
    "            'merge_duration': merge_duration,\n",
    "            'merged_file_size': merged_file_size,\n",
    "            'individual_files': individual_files,\n",
    "            'individual_sizes': individual_sizes\n",
    "        })\n",
    "\n",
    "def read_and_decrypt_files(folder_path_base64, folder_path_merged, merge_report, decryption_report):\n",
    "    if not os.path.exists(folder_path_merged):\n",
    "        os.makedirs(folder_path_merged)\n",
    "\n",
    "    user_data = {}\n",
    "\n",
    "    for filename in os.listdir(folder_path_base64):\n",
    "        if filename.endswith('_base64.txt'):\n",
    "            file_path = os.path.join(folder_path_base64, filename)\n",
    "            file_size = os.path.getsize(file_path)\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                encoded_data = file.read()\n",
    "                padded_encoded_data = encoded_data + '==='\n",
    "                try:\n",
    "                    decoded_data = base64.b64decode(padded_encoded_data).decode('utf-8')\n",
    "                    decrypted_info, user_id = decrypt_ct_prime(decoded_data, decryption_report, filename)  # Capture user_id\n",
    "\n",
    "                    timestamp = datetime.strptime(decrypted_info['timestamp'], \"%Y-%m-%d %H_%M_%S\")\n",
    "                    if user_id not in user_data:\n",
    "                        user_data[user_id] = []\n",
    "                    decrypted_info['filename'] = filename\n",
    "                    decrypted_info['file_size'] = file_size\n",
    "                    user_data[user_id].append((timestamp, decrypted_info))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error decoding file {filename}: {e}\")\n",
    "\n",
    "    for user_id, data in user_data.items():\n",
    "        data.sort(key=lambda x: x[0])  # Sort by timestamp\n",
    "        merged_data = []\n",
    "        current_batch = []\n",
    "        current_batch_time = None\n",
    "\n",
    "        for timestamp, info in data:\n",
    "            if current_batch_time is None or timestamp - current_batch_time <= timedelta(minutes=20):\n",
    "                current_batch.append(info)\n",
    "            else:\n",
    "                merged_data.append(current_batch)\n",
    "                current_batch = [info]\n",
    "            current_batch_time = timestamp\n",
    "\n",
    "        if current_batch:\n",
    "            merged_data.append(current_batch)\n",
    "\n",
    "        save_merged_data(merged_data, user_id, folder_path_merged, merge_report)\n",
    "\n",
    "def write_merge_report(merge_report, report_path):\n",
    "    merge_report_path = os.path.join(report_path, \"merge_report.csv\")\n",
    "    with open(merge_report_path, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['user_id', 'merged_file', 'merge_duration', 'merged_file_size', 'individual_files', 'individual_sizes'])\n",
    "        writer.writeheader()\n",
    "        for entry in merge_report:\n",
    "            writer.writerow(entry)\n",
    "\n",
    "def write_decryption_report(decryption_report, report_path):\n",
    "    decryption_report_path = os.path.join(report_path, \"decryption_report.csv\")\n",
    "    with open(decryption_report_path, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['filename', 'user_id', 'decryption_duration'])\n",
    "        writer.writeheader()\n",
    "        for entry in decryption_report:\n",
    "            writer.writerow(entry)\n",
    "\n",
    "def main():\n",
    "    folder_path_base64 = input(\"Enter the folder path containing Base64 encoded files for decryption: \")\n",
    "    folder_path_merged = input(\"Enter the folder path to save merged data: \")\n",
    "    report_path = input(\"Enter the directory for the reports: \")\n",
    "\n",
    "    merge_report = []\n",
    "    decryption_report = []\n",
    "    read_and_decrypt_files(folder_path_base64, folder_path_merged, merge_report, decryption_report)\n",
    "    write_merge_report(merge_report, report_path)\n",
    "    write_decryption_report(decryption_report, report_path)\n",
    "\n",
    "    print(f\"Merged files saved in {folder_path_merged}\")\n",
    "    print(f\"Reports saved in {report_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf69424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c68318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab553cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d64f88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bda39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df57d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26322801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f2f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5349b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef85ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab36954a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c3d4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461362f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad0aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04be7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371a06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
